{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lot_run_msl.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMc+OC8BKk3UVMk2kW5bTcA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from google.colab import drive    # Doesn't work outside CoLab!\n","drive.mount(\"/content/My_Laptop\", force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7RnGKA5FzXLX","executionInfo":{"status":"ok","timestamp":1659659961896,"user_tz":420,"elapsed":17236,"user":{"displayName":"Eric Huang","userId":"05751137817818220931"}},"outputId":"9f45bc92-d952-485e-a2a8-552fdec5cf77"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/My_Laptop\n"]}]},{"cell_type":"code","source":["cd /content/My_Laptop/Othercomputers/My Laptop/NLP/LOTClass"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2JMoMqzo7SgZ","executionInfo":{"status":"ok","timestamp":1659659980242,"user_tz":420,"elapsed":145,"user":{"displayName":"Eric Huang","userId":"05751137817818220931"}},"outputId":"59d7ed05-5c0b-430b-f94a-ee716df7d396"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/My_Laptop/Othercomputers/My Laptop/NLP/LOTClass\n"]}]},{"cell_type":"code","source":["!pip3 install -r requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"DHdq1mBCpdtP","executionInfo":{"status":"ok","timestamp":1659660106510,"user_tz":420,"elapsed":117217,"user":{"displayName":"Eric Huang","userId":"05751137817818220931"}},"outputId":"402dc171-9f5c-4799-f434-a8103f783d8a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torch==1.5.0\n","  Downloading torch-1.5.0-cp37-cp37m-manylinux1_x86_64.whl (752.0 MB)\n","\u001b[K     |████████████████████████████████| 752.0 MB 10 kB/s \n","\u001b[?25hCollecting transformers==3.3.1\n","  Downloading transformers-3.3.1-py3-none-any.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 52.3 MB/s \n","\u001b[?25hCollecting joblib==0.16.0\n","  Downloading joblib-0.16.0-py3-none-any.whl (300 kB)\n","\u001b[K     |████████████████████████████████| 300 kB 75.2 MB/s \n","\u001b[?25hCollecting nltk==3.5\n","  Downloading nltk-3.5.zip (1.4 MB)\n","\u001b[K     |████████████████████████████████| 1.4 MB 52.5 MB/s \n","\u001b[?25hCollecting numpy==1.18.5\n","  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n","\u001b[K     |████████████████████████████████| 20.1 MB 1.2 MB/s \n","\u001b[?25hCollecting tqdm==4.47.0\n","  Downloading tqdm-4.47.0-py2.py3-none-any.whl (66 kB)\n","\u001b[K     |████████████████████████████████| 66 kB 6.4 MB/s \n","\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.5.0->-r requirements.txt (line 1)) (0.16.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->-r requirements.txt (line 2)) (21.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->-r requirements.txt (line 2)) (3.7.1)\n","Collecting sentencepiece!=0.1.92\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 57.6 MB/s \n","\u001b[?25hCollecting tokenizers==0.8.1.rc2\n","  Downloading tokenizers-0.8.1rc2-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n","\u001b[K     |████████████████████████████████| 3.0 MB 49.8 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->-r requirements.txt (line 2)) (2.23.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[K     |████████████████████████████████| 880 kB 78.4 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->-r requirements.txt (line 2)) (2022.6.2)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk==3.5->-r requirements.txt (line 4)) (7.1.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.3.1->-r requirements.txt (line 2)) (3.0.9)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.1->-r requirements.txt (line 2)) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.1->-r requirements.txt (line 2)) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.1->-r requirements.txt (line 2)) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.1->-r requirements.txt (line 2)) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.3.1->-r requirements.txt (line 2)) (1.15.0)\n","Building wheels for collected packages: nltk, sacremoses\n","  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434690 sha256=afc389980a9448760430311c15e8958335886731efe155dfc3d8ace4818f6534\n","  Stored in directory: /root/.cache/pip/wheels/45/6c/46/a1865e7ba706b3817f5d1b2ff7ce8996aabdd0d03d47ba0266\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=b0a3529424d833c56afa5294d32dd047c042d2f520ffd42aca99296ae72fe7bf\n","  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n","Successfully built nltk sacremoses\n","Installing collected packages: tqdm, joblib, tokenizers, sentencepiece, sacremoses, numpy, transformers, torch, nltk\n","  Attempting uninstall: tqdm\n","    Found existing installation: tqdm 4.64.0\n","    Uninstalling tqdm-4.64.0:\n","      Successfully uninstalled tqdm-4.64.0\n","  Attempting uninstall: joblib\n","    Found existing installation: joblib 1.1.0\n","    Uninstalling joblib-1.1.0:\n","      Successfully uninstalled joblib-1.1.0\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.21.6\n","    Uninstalling numpy-1.21.6:\n","      Successfully uninstalled numpy-1.21.6\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.12.0+cu113\n","    Uninstalling torch-1.12.0+cu113:\n","      Successfully uninstalled torch-1.12.0+cu113\n","  Attempting uninstall: nltk\n","    Found existing installation: nltk 3.7\n","    Uninstalling nltk-3.7:\n","      Successfully uninstalled nltk-3.7\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.18.5 which is incompatible.\n","torchvision 0.13.0+cu113 requires torch==1.12.0, but you have torch 1.5.0 which is incompatible.\n","torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.5.0 which is incompatible.\n","torchaudio 0.12.0+cu113 requires torch==1.12.0, but you have torch 1.5.0 which is incompatible.\n","tensorflow 2.8.2+zzzcolab20220719082949 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\n","tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.18.5 which is incompatible.\n","panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.47.0 which is incompatible.\n","jaxlib 0.3.14+cuda11.cudnn805 requires numpy>=1.19, but you have numpy 1.18.5 which is incompatible.\n","jax 0.3.14 requires numpy>=1.19, but you have numpy 1.18.5 which is incompatible.\n","fastai 2.7.7 requires torch<1.13,>=1.7, but you have torch 1.5.0 which is incompatible.\n","cmdstanpy 1.0.4 requires numpy>=1.21, but you have numpy 1.18.5 which is incompatible.\u001b[0m\n","Successfully installed joblib-0.16.0 nltk-3.5 numpy-1.18.5 sacremoses-0.0.53 sentencepiece-0.1.96 tokenizers-0.8.1rc2 torch-1.5.0 tqdm-4.47.0 transformers-3.3.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]}}},"metadata":{}}]},{"cell_type":"markdown","source":["It seems like changing it to bert-base-chinese is doable"],"metadata":{"id":"9XUYD9uutj36"}},{"cell_type":"code","source":["!bash train.sh"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6KblEfbL7Gb5","executionInfo":{"status":"ok","timestamp":1658454613595,"user_tz":420,"elapsed":419786,"user":{"displayName":"Eric Huang","userId":"05751137817818220931"}},"outputId":"05cc653b-61fc-4b35-f757-308a718aea32"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Namespace(accum_steps=4, category_vocab_size=100, dataset_dir='datasets/my_train/', dist_port=12345, early_stop=False, eval_batch_size=128, final_model='final_model.pt', gpus=1, label_names_file='label_names.txt', match_threshold=20, max_len=200, mcp_epochs=3, out_file='out.txt', self_train_epochs=1.0, test_file='test.txt', test_label_file='test_labels.txt', top_pred_num=50, train_batch_size=32, train_file='train.txt', update_interval=50)\n","Effective training batch size: 128\n","Label names used for each class are: {0: ['寶鈔', '鈔'], 1: ['白銀', '銀'], 2: ['賦稅', '稅'], 3: ['貢賞', '貢'], 4: ['貿', '商', '海']}\n","Some weights of the model checkpoint at bert-base-chinese were not used when initializing LOTClassModel: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing LOTClassModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing LOTClassModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of LOTClassModel were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['cls.predictions.decoder.bias', 'dense.weight', 'dense.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Reading texts from datasets/my_train/train.txt\n","Converting texts into tensors.\n","Saving encoded texts into datasets/my_train/train.pt\n","Reading texts from datasets/my_train/train.txt\n","Locating label names in the corpus.\n","Saving texts with label names into datasets/my_train/label_name_data.pt\n","Loading encoded texts from datasets/my_train/test.pt\n","Contructing category vocabulary.\n","100% 32/32 [01:09<00:00,  2.16s/it]\n","Class 0 category vocabulary: ['鈔', '钞', '元', '楮', '綬', '綵', '俸', '彩', '冊', '二', '敕', '誥', '股', '緞', '錠', '匯', '秩', '卷', '璽', '滙', '茶', '綢', '仟', '箋', '枚', '票', '等', '臘', '圜', '紫', '顏', '咸', '夾', '袋', '乞', '有', '褂']\n","\n","Class 1 category vocabulary: ['銀', '银', '玉', '條', '糖', '錫', '白', '字', '鎳', '四', '鍊', '鐘', '棗', '寶', '月', '織', '針', '紅', '酒', '資', '骨', '薪', '械', '蘇', '言', '劍', '磚', '頭', '餘', '牌', '盧', '革', '煉', '料', '分', '綠', '名', '子', '屬', '花', '克', '董', '鋅', '鈀', '鉛', '總', '電', '雕', '皮', '長', '黨']\n","\n","Class 2 category vocabulary: ['稅', '税', '納', '租', '費', '課', '庫', '繳', '雜', '收', '戶', '谷', '吏', '額', '縣', '所', '征', '菸', '刑', '民', '榷', '舊', '漕', '實', '年', '墾', '利', '領', '免', '政', '檢', '法', '禁', '調', '津', '司', '畝', '廳', '稽', '關', '典', '監', '業', '抵', '務', '事', '徴', '計']\n","\n","Class 3 category vocabulary: ['貢', '贡', '朝', '賜', '進', '用', '讓', '呈', '出', '臣', '產', '請', '賞', '質', '奉', '供', '藩', '古', '換', '表', '奏', '見', '廷', '降', '買', '賀', '貿', '稟', '賣', '犒', '試', '諸', '參', '越', '漢', '從', '丹', '提', '奴', '献', '派', '邊', '侵', '侯', '輸', '代', '求', '售', '與', '級', '祭', '外', '將', '謝', '訪', '接', '本', '選']\n","\n","Class 4 category vocabulary: ['海', '洋', '山', '江', '陸', '水', '天', '地', '島', '河', '沙', '土', '港', '船', '南', '半', '無', '灣', '遠', '小', '浪', '多', '中', '商', '少', '上', '航', '其', '艦', '阿', '泰', '岸', '網', '萊', '北', '尼', '空', '工', '世', '濟', '漁', '離', '魚', '太', '農', '平', '濤', '在', '於', '淵', '者', '灘', '建', '省', '販', '帆', '路', '內', '雷', '台', '行', '境', '周', '龍', '風', '公', '東', '崖']\n","\n","Preparing self supervision for masked category prediction.\n","100% 35/35 [01:20<00:00,  2.31s/it]\n","Number of documents with category indicative terms found for each category is: {0: 33, 1: 150, 2: 94, 3: 1265, 4: 302}\n","There are totally 1844 documents with category indicative terms.\n","\n","Training model via masked category prediction.\n","Epoch 1:\n","100% 58/58 [01:10<00:00,  1.22s/it]\n","Average training loss: 1.0732899904251099\n","Epoch 2:\n","100% 58/58 [01:10<00:00,  1.22s/it]\n","Average training loss: 0.37382474541664124\n","Epoch 3:\n","100% 58/58 [01:10<00:00,  1.22s/it]\n","Average training loss: 0.17668184638023376\n","\n","Start self-training.\n","<bound method LOTClassTrainer.self_train_dist of <trainer.LOTClassTrainer object at 0x7f4fe5863050>>\n","1\n","Traceback (most recent call last):\n","  File \"src/train.py\", line 66, in <module>\n","    main()\n","  File \"src/train.py\", line 59, in main\n","    trainer.self_train(epochs=args.self_train_epochs, loader_name=args.final_model)\n","  File \"/content/drive/MyDrive/NLP/LOTClass/src/trainer.py\", line 582, in self_train\n","    mp.spawn(self.self_train_dist, nprocs=self.world_size, args=(epochs, loader_name))\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\", line 200, in spawn\n","    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\", line 158, in start_processes\n","    while not context.join():\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\", line 119, in join\n","    raise Exception(msg)\n","Exception: \n","\n","-- Process 0 terminated with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\", line 20, in _wrap\n","    fn(i, *args)\n","  File \"/content/drive/MyDrive/NLP/LOTClass/src/trainer.py\", line 546, in self_train_dist\n","    test_dataset_loader = self.make_dataloader(rank, self.test_data, self.eval_batch_size) if self.with_test_label else None\n","  File \"/content/drive/MyDrive/NLP/LOTClass/src/trainer.py\", line 224, in make_dataloader\n","    dataset = TensorDataset(data_dict[\"input_ids\"], data_dict[\"attention_masks\"], data_dict[\"labels\"])\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataset.py\", line 158, in __init__\n","    assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n","AssertionError\n","\n"]}]},{"cell_type":"code","source":["!bash train.sh"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659059573102,"user_tz":420,"elapsed":397101,"user":{"displayName":"Eric Huang","userId":"05751137817818220931"}},"outputId":"21cf56d5-90e2-4d9c-ddf6-ef92a1a1e0de","id":"OEqJs-SzPjJT"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Namespace(accum_steps=4, category_vocab_size=100, dataset_dir='datasets/my_train/', dist_port=12345, early_stop=False, eval_batch_size=128, final_model='final_model.pt', gpus=1, label_names_file='label_names.txt', match_threshold=20, max_len=200, mcp_epochs=3, out_file='out.txt', self_train_epochs=1.0, test_file='test.txt', test_label_file='test_labels.txt', top_pred_num=50, train_batch_size=32, train_file='cleaned_train.txt', update_interval=50)\n","Effective training batch size: 128\n","Label names used for each class are: {0: ['寶鈔', '鈔', '元'], 1: ['白銀', '銀'], 2: ['賦稅', '稅', '繳', '收'], 3: ['貢', '贡', '進', '朝'], 4: ['貿', '商', '買', '賣', '販']}\n","Some weights of the model checkpoint at bert-base-chinese were not used when initializing LOTClassModel: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing LOTClassModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing LOTClassModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of LOTClassModel were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['cls.predictions.decoder.bias', 'dense.weight', 'dense.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Reading texts from datasets/my_train/cleaned_train.txt\n","Converting texts into tensors.\n","Saving encoded texts into datasets/my_train/train.pt\n","Reading texts from datasets/my_train/cleaned_train.txt\n","Locating label names in the corpus.\n","Saving texts with label names into datasets/my_train/label_name_data.pt\n","Loading encoded texts from datasets/my_train/test.pt\n","Contructing category vocabulary.\n","100% 42/42 [01:29<00:00,  2.13s/it]\n","Class 0 category vocabulary: ['元', '鈔', '钞', '楮', '綵', '綬', '俸', '冊', '股', '二', '千', '敕', '券', '匯', '誥', '仟', '錠', '滙', '正', '票', '卷', '夾', '璽', '秩', '時', '咸', '綢', '枚', '箋', '紫', '十', '乞', '顏', '臘', '等']\n","\n","Class 1 category vocabulary: ['銀', '鐵', '银', '白', '玉', '石', '錫', '條', '字', '鎳', '寶', '紅', '棗', '鐘', '四', '器', '月', '青', '盧', '織', '薪', '資', '鍊', '骨', '名', '磚', '革', '紳', '硬', '雲', '皮', '餘', '頭', '綠', '針', '長', '言', '劍', '料', '械', '董', '牌', '黨', '電', '冰', '蘇', '萬', '磁', '棉']\n","\n","Class 2 category vocabulary: ['稅', '繳', '收', '税', '田', '役', '賦', '課', '補', '所', '庫', '歲', '門', '谷', '吉', '雜', '海', '征', '縣', '免', '吏', '領', '利', '刑', '収', '檢', '菸', '調', '扣', '戶', '年', '付', '放', '發', '額', '受', '舊', '計', '政', '墾', '關', '分', '減', '抵', '播', '存', '開', '實', '理', '廣', '士', '值', '榖']\n","\n","Class 3 category vocabulary: ['朝', '貢', '進', '請', '廷', '獻', '往', '見', '呈', '贡', '代', '漢', '讓', '鮮', '晉', '賜', '邊', '都', '外', '參', '從', '向', '和', '明', '表', '換', '南', '越', '贈', '試', '藩', '庭', '去', '帝', '質', '求', '臣', '賀', '賞', '高', '中', '到', '拜', '還', '古', '本', '陳', '清', '降', '來', '城', '天', '西', '宮', '遣', '日', '稟', '奏', '上', '奉', '問', '世']\n","\n","Class 4 category vocabulary: ['商', '販', '買', '農', '工', '賣', '廠', '私', '食', '購', '市', '同', '公', '良', '辦', '售', '好', '為', '礦', '者', '貿', '洋', '郵', '漁', '偷', '船', '銷', '企', '鄭', '自', '买', '業', '輸', '眾', '創', '餅', '吃', '甲', '玩', '議', '遊', '殺', '王', '盜', '小', '看', '榷']\n","\n","Preparing self supervision for masked category prediction.\n","100% 45/45 [01:30<00:00,  2.00s/it]\n","Number of documents with category indicative terms found for each category is: {0: 27, 1: 101, 2: 211, 3: 1285, 4: 22}\n","There are totally 1646 documents with category indicative terms.\n","\n","Training model via masked category prediction.\n","Epoch 1:\n","100% 52/52 [00:53<00:00,  1.03s/it]\n","Average training loss: 1.0428253412246704\n","Epoch 2:\n","100% 52/52 [00:54<00:00,  1.04s/it]\n","Average training loss: 0.256452351808548\n","Epoch 3:\n","100% 52/52 [00:54<00:00,  1.05s/it]\n","Average training loss: 0.1284019947052002\n","\n","Start self-training.\n","<bound method LOTClassTrainer.self_train_dist of <trainer.LOTClassTrainer object at 0x7fc54ef45810>>\n","1\n","Traceback (most recent call last):\n","  File \"src/train.py\", line 66, in <module>\n","    main()\n","  File \"src/train.py\", line 59, in main\n","    trainer.self_train(epochs=args.self_train_epochs, loader_name=args.final_model)\n","  File \"/content/drive/MyDrive/NLP/LOTClass/src/trainer.py\", line 582, in self_train\n","    mp.spawn(self.self_train_dist, nprocs=self.world_size, args=(epochs, loader_name))\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\", line 200, in spawn\n","    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\", line 158, in start_processes\n","    while not context.join():\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\", line 119, in join\n","    raise Exception(msg)\n","Exception: \n","\n","-- Process 0 terminated with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\", line 20, in _wrap\n","    fn(i, *args)\n","  File \"/content/drive/MyDrive/NLP/LOTClass/src/trainer.py\", line 546, in self_train_dist\n","    test_dataset_loader = self.make_dataloader(rank, self.test_data, self.eval_batch_size) if self.with_test_label else None\n","  File \"/content/drive/MyDrive/NLP/LOTClass/src/trainer.py\", line 224, in make_dataloader\n","    dataset = TensorDataset(data_dict[\"input_ids\"], data_dict[\"attention_masks\"], data_dict[\"labels\"])\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataset.py\", line 158, in __init__\n","    assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n","AssertionError\n","\n"]}]},{"cell_type":"code","source":["!bash train.sh"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bWKhMl8fAIyS","executionInfo":{"status":"ok","timestamp":1659660949929,"user_tz":420,"elapsed":804271,"user":{"displayName":"Eric Huang","userId":"05751137817818220931"}},"outputId":"f4ceba85-1f39-4b8f-a1e0-1c3e9c6ecd75"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Namespace(accum_steps=4, category_vocab_size=100, dataset_dir='datasets/my_train/', dist_port=12345, early_stop=False, eval_batch_size=128, final_model='final_model.pt', gpus=1, label_names_file='label_names.txt', match_threshold=20, max_len=200, mcp_epochs=3, out_file='out.txt', self_train_epochs=1.0, test_file='test.txt', test_label_file='test_labels.txt', top_pred_num=50, train_batch_size=32, train_file='cleaned_train.txt', update_interval=50)\n","Effective training batch size: 128\n","Downloading: 100% 110k/110k [00:00<00:00, 878kB/s] \n","Label names used for each class are: {0: ['寶鈔', '鈔', '元'], 1: ['白銀', '銀'], 2: ['賦稅', '稅', '繳', '收'], 3: ['貢', '贡', '進', '朝'], 4: ['貿', '商', '買', '賣', '販']}\n","Downloading: 100% 624/624 [00:00<00:00, 553kB/s]\n","Downloading: 100% 412M/412M [00:10<00:00, 40.6MB/s]\n","Some weights of the model checkpoint at bert-base-chinese were not used when initializing LOTClassModel: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing LOTClassModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing LOTClassModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of LOTClassModel were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['cls.predictions.decoder.bias', 'dense.weight', 'dense.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Reading texts from datasets/my_train/cleaned_train.txt\n","Converting texts into tensors.\n","Saving encoded texts into datasets/my_train/train.pt\n","Reading texts from datasets/my_train/cleaned_train.txt\n","Locating label names in the corpus.\n","Saving texts with label names into datasets/my_train/label_name_data.pt\n","Reading texts from datasets/my_train/test.txt\n","Converting texts into tensors.\n","Saving encoded texts into datasets/my_train/test.pt\n","Reading labels from datasets/my_train/test_labels.txt\n","Contructing category vocabulary.\n"," 66% 245/370 [10:07<05:13,  2.51s/it]Traceback (most recent call last):\n","  File \"src/train.py\", line 66, in <module>\n","    main()\n","  File \"src/train.py\", line 55, in main\n","    trainer.category_vocabulary(top_pred_num=args.top_pred_num, category_vocab_size=args.category_vocab_size)\n","  File \"/content/My_Laptop/Othercomputers/My Laptop/NLP/LOTClass/src/trainer.py\", line 309, in category_vocabulary\n","    mp.spawn(self.category_vocabulary_dist, nprocs=self.world_size, args=(top_pred_num, loader_name))\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\", line 200, in spawn\n","    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\", line 158, in start_processes\n","    while not context.join():\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\", line 78, in join\n","    timeout=timeout,\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 921, in wait\n","    ready = selector.select(timeout)\n","  File \"/usr/lib/python3.7/selectors.py\", line 415, in select\n","    fd_event_list = self._selector.poll(timeout)\n","KeyboardInterrupt\n"," 66% 245/370 [10:09<05:10,  2.49s/it]\n"]}]},{"cell_type":"code","source":["!bash train.sh"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pVPMlH0qgQQ5","executionInfo":{"status":"ok","timestamp":1659662938401,"user_tz":420,"elapsed":1469215,"user":{"displayName":"Eric Huang","userId":"05751137817818220931"}},"outputId":"ca5b8d48-de20-450d-f256-88dc6601cab4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Namespace(accum_steps=4, category_vocab_size=100, dataset_dir='datasets/my_train/', dist_port=12345, early_stop=False, eval_batch_size=128, final_model='final_model.pt', gpus=1, label_names_file='label_names.txt', match_threshold=20, max_len=200, mcp_epochs=3, out_file='out.txt', self_train_epochs=1.0, test_file='test.txt', test_label_file='test_labels.txt', top_pred_num=50, train_batch_size=32, train_file='cleaned_train.txt', update_interval=50)\n","Effective training batch size: 128\n","Label names used for each class are: {0: ['寶鈔', '鈔', '元'], 1: ['白銀', '銀'], 2: ['賦稅', '稅', '繳', '收'], 3: ['貢', '贡', '進', '朝'], 4: ['貿', '商', '買', '賣', '販']}\n","Some weights of the model checkpoint at bert-base-chinese were not used when initializing LOTClassModel: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing LOTClassModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing LOTClassModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of LOTClassModel were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['cls.predictions.decoder.bias', 'dense.weight', 'dense.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Loading encoded texts from datasets/my_train/train.pt\n","Loading texts with label names from datasets/my_train/label_name_data.pt\n","Loading encoded texts from datasets/my_train/test.pt\n","Contructing category vocabulary.\n","100% 370/370 [15:21<00:00,  2.49s/it]\n","Class 0 category vocabulary: ['元', '鈔', '钞', '楮', '綵', '綬', '文', '春', '仁', '千', '股', '冊', '宋', '百', '義', '彩', '初', '匯', '詔', '新', '、', '次', '太', '緞', '敕', '宣', '其', '粟', '六', '仟', '滙', '安', '票', '五', '布', '夾', '先', '八', '帛', '化', '誥', '七']\n","\n","Class 1 category vocabulary: ['銀', '銅', '银', '鐵', '款', '白', '條', '月', '資', '餘', '薪', '錫', '帳', '字', '玉', '賑', '石', '黃', '花', '至', '祿', '頭', '子', '料', '黨', '貸', '名', '寶', '盧', '鎳', '紅', '萬', '棗', '青', '總', '甘', '爾', '項', '紳', '九', '雲', '織', '鍊', '局', '革']\n","\n","Class 2 category vocabulary: ['收', '繳', '稅', '収', '領', '所', '管', '檢', '税', '放', '免', '州', '調', '受', '扣', '役', '利', '田', '付', '賦', '捐', '補', '課', '接', '門', '谷', '理', '海', '征', '機', '雜', '播', '吉', '縣', '待', '查', '菸', '關', '計', '吏', '生', '合', '刑', '統', '持', '漕', '保', '併', '藏']\n","\n","Class 3 category vocabulary: ['進', '朝', '貢', '廷', '舉', '上', '獻', '代', '呈', '往', '來', '後', '贡', '試', '參', '口', '讓', '退', '賜', '晉', '與', '帝', '日', '都', '薦', '賞', '外', '庭', '前', '登', '從', '漢', '道', '主', '邊', '贈', '藩', '可', '內', '宮', '高', '臣', '进', '實', '表', '向', '鮮', '起', '南', '世']\n","\n","Class 4 category vocabulary: ['買', '商', '賣', '購', '販', '售', '廠', '貨', '私', '農', '營', '买', '工', '市', '辦', '看', '偷', '吃', '銷', '訂', '殺', '者', '玩', '食', '貿', '自', '養', '物', '洋', '省', '會', '搶', '企', '船', '產', '寫', '郵', '做', '輸', '裝', '價', '置', '礦', '好', '員', '借', '業', '盜', '攤', '造']\n","\n","Preparing self supervision for masked category prediction.\n","100% 229/229 [08:42<00:00,  2.28s/it]\n","Number of documents with category indicative terms found for each category is: {0: 0, 1: 613, 2: 268, 3: 529, 4: 29}\n","Traceback (most recent call last):\n","  File \"src/train.py\", line 66, in <module>\n","    main()\n","  File \"src/train.py\", line 57, in main\n","    trainer.mcp(top_pred_num=args.top_pred_num, match_threshold=args.match_threshold, epochs=args.mcp_epochs)\n","  File \"/content/My_Laptop/Othercomputers/My Laptop/NLP/LOTClass/src/trainer.py\", line 465, in mcp\n","    self.prepare_mcp(top_pred_num, match_threshold)\n","  File \"/content/My_Laptop/Othercomputers/My Laptop/NLP/LOTClass/src/trainer.py\", line 406, in prepare_mcp\n","    assert category_doc_num[i] > 10, f\"Too few ({category_doc_num[i]}) documents with category indicative terms found for category {i}; \" \\\n","AssertionError: Too few (0) documents with category indicative terms found for category 0; try to add more unlabeled documents to the training corpus (recommend) or reduce `--match_threshold` (not recommend)\n"]}]},{"cell_type":"code","source":["!bash train.sh"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TrphueToq3Fj","executionInfo":{"status":"ok","timestamp":1659663953229,"user_tz":420,"elapsed":177584,"user":{"displayName":"Eric Huang","userId":"05751137817818220931"}},"outputId":"5d9d7d96-3020-4aa8-f010-70858ac5e508"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Namespace(accum_steps=4, category_vocab_size=100, dataset_dir='datasets/my_train/', dist_port=12345, early_stop=False, eval_batch_size=128, final_model='final_model.pt', gpus=1, label_names_file='label_names.txt', match_threshold=20, max_len=200, mcp_epochs=3, out_file='out.txt', self_train_epochs=1.0, test_file='test.txt', test_label_file='test_labels.txt', top_pred_num=50, train_batch_size=32, train_file='cleaned_train.txt', update_interval=50)\n","Effective training batch size: 128\n","Label names used for each class are: {0: ['寶鈔', '鈔', '元'], 1: ['白銀', '銀'], 2: ['賦稅', '稅', '繳', '收'], 3: ['貢', '贡', '進', '朝'], 4: ['貿', '商', '買', '賣', '販']}\n","Some weights of the model checkpoint at bert-base-chinese were not used when initializing LOTClassModel: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing LOTClassModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing LOTClassModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of LOTClassModel were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['cls.predictions.decoder.bias', 'dense.weight', 'dense.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Loading encoded texts from datasets/my_train/train.pt\n","Loading texts with label names from datasets/my_train/label_name_data.pt\n","Loading encoded texts from datasets/my_train/test.pt\n","Loading category vocabulary from datasets/my_train/category_vocab.pt\n","Class 0 category vocabulary: ['元', '鈔', '钞', '楮', '綵', '綬', '文', '春', '仁', '千', '股', '冊', '宋', '百', '義', '彩', '初', '匯', '詔', '新', '、', '次', '太', '緞', '敕', '宣', '其', '粟', '六', '仟', '滙', '安', '票', '五', '布', '夾', '先', '八', '帛', '化', '誥', '七']\n","\n","Class 1 category vocabulary: ['銀', '銅', '银', '鐵', '款', '白', '條', '月', '資', '餘', '薪', '錫', '帳', '字', '玉', '賑', '石', '黃', '花', '至', '祿', '頭', '子', '料', '黨', '貸', '名', '寶', '盧', '鎳', '紅', '萬', '棗', '青', '總', '甘', '爾', '項', '紳', '九', '雲', '織', '鍊', '局', '革']\n","\n","Class 2 category vocabulary: ['收', '繳', '稅', '収', '領', '所', '管', '檢', '税', '放', '免', '州', '調', '受', '扣', '役', '利', '田', '付', '賦', '捐', '補', '課', '接', '門', '谷', '理', '海', '征', '機', '雜', '播', '吉', '縣', '待', '查', '菸', '關', '計', '吏', '生', '合', '刑', '統', '持', '漕', '保', '併', '藏']\n","\n","Class 3 category vocabulary: ['進', '朝', '貢', '廷', '舉', '上', '獻', '代', '呈', '往', '來', '後', '贡', '試', '參', '口', '讓', '退', '賜', '晉', '與', '帝', '日', '都', '薦', '賞', '外', '庭', '前', '登', '從', '漢', '道', '主', '邊', '贈', '藩', '可', '內', '宮', '高', '臣', '进', '實', '表', '向', '鮮', '起', '南', '世']\n","\n","Class 4 category vocabulary: ['買', '商', '賣', '購', '販', '售', '廠', '貨', '私', '農', '營', '买', '工', '市', '辦', '看', '偷', '吃', '銷', '訂', '殺', '者', '玩', '食', '貿', '自', '養', '物', '洋', '省', '會', '搶', '企', '船', '產', '寫', '郵', '做', '輸', '裝', '價', '置', '礦', '好', '員', '借', '業', '盜', '攤', '造']\n","\n","Loading masked category prediction data from datasets/my_train/mcp_train.pt\n","There are totally 1439 documents with category indicative terms.\n","\n","Training model via masked category prediction.\n","Epoch 1:\n","100% 45/45 [00:46<00:00,  1.02s/it]\n","Average training loss: 1.1089860200881958\n","Epoch 2:\n","100% 45/45 [00:50<00:00,  1.12s/it]\n","Average training loss: 0.32566455006599426\n","Epoch 3:\n","100% 45/45 [00:51<00:00,  1.14s/it]\n","Average training loss: 0.17132340371608734\n","\n","Start self-training.\n","<bound method LOTClassTrainer.self_train_dist of <trainer.LOTClassTrainer object at 0x7efbeaf14f90>>\n","1\n","Traceback (most recent call last):\n","  File \"src/train.py\", line 66, in <module>\n","    main()\n","  File \"src/train.py\", line 59, in main\n","    trainer.self_train(epochs=args.self_train_epochs, loader_name=args.final_model)\n","  File \"/content/My_Laptop/Othercomputers/My Laptop/NLP/LOTClass/src/trainer.py\", line 582, in self_train\n","    mp.spawn(self.self_train_dist, nprocs=self.world_size, args=(epochs, loader_name))\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\", line 200, in spawn\n","    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\", line 158, in start_processes\n","    while not context.join():\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\", line 119, in join\n","    raise Exception(msg)\n","Exception: \n","\n","-- Process 0 terminated with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\", line 20, in _wrap\n","    fn(i, *args)\n","  File \"/content/My_Laptop/Othercomputers/My Laptop/NLP/LOTClass/src/trainer.py\", line 546, in self_train_dist\n","    test_dataset_loader = self.make_dataloader(rank, self.test_data, self.eval_batch_size) if self.with_test_label else None\n","  File \"/content/My_Laptop/Othercomputers/My Laptop/NLP/LOTClass/src/trainer.py\", line 224, in make_dataloader\n","    dataset = TensorDataset(data_dict[\"input_ids\"], data_dict[\"attention_masks\"], data_dict[\"labels\"])\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataset.py\", line 158, in __init__\n","    assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n","AssertionError\n","\n"]}]}]}